{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOW1_zyoCCQA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EJuO2mDEDEY"
      },
      "outputs": [],
      "source": [
        "# Input Embeddings\n",
        "\n",
        "class InputEmbeddings(nn.Module):\n",
        "  def __init__(self, d_model: int, vocab_size: int):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embeddings = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.embeddings(x) * math.sqrt(self.d_model)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_5tdT6QKQyn",
        "outputId": "6c364207-c395-45c8-f020-bd8148c8e390"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding(10, 4)\n",
            "tensor([[-1.5787,  3.0195,  3.2364,  0.4048],\n",
            "        [ 3.1898, -2.2366,  0.4090,  1.7561]], grad_fn=<MulBackward0>)\n",
            "torch.Size([2, 4])\n"
          ]
        }
      ],
      "source": [
        "# Testing/Visualizing InputEmbeddings Functionality\n",
        "\n",
        "# hypothetical vocab size, and d_model\n",
        "ex_vocab_size = 10\n",
        "dim_model = 4\n",
        "\n",
        "_embeddings = InputEmbeddings(d_model=dim_model, vocab_size=ex_vocab_size)\n",
        "print(_embeddings.embeddings)\n",
        "\n",
        "# sentance with token ID 1,2\n",
        "tokenized_sentence = torch.LongTensor([1, 2])\n",
        "\n",
        "# Get embeddings for the tokenized sentence\n",
        "embedded_sentence = _embeddings(tokenized_sentence)\n",
        "print(embedded_sentence)\n",
        "print(embedded_sentence.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6DV2OW8KU0y"
      },
      "outputs": [],
      "source": [
        "# Positional Encoding\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    # max length of the sentance (need to create one vector for each position)\n",
        "    self.seq_len = seq_len\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # matrix of shape (seq_len, dim model)\n",
        "    pos_enc = torch.zeros(seq_len, d_model)\n",
        "\n",
        "    # vector for seq_len\n",
        "    position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0)/ d_model))\n",
        "\n",
        "    # from paper, sin to even positions\n",
        "    pos_enc[:, 0::2] = torch.sin(position * div_term)\n",
        "    pos_enc[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "    # we will have a batch of sentances, so we need a new dimension\n",
        "    pos_enc = pos_enc.unsqueeze(0)\n",
        "\n",
        "    self.register_buffer(\"pos_enc\", pos_enc)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # pos enc to every word in sentance\n",
        "    x = x + (self.pos_enc[:, :x.shape[1], :]).requires_grad_(False)\n",
        "    return self.dropout(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsKhLbXfx3hh",
        "outputId": "dfc68015-4dfe-4ba7-a179-4d9dc2306bc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 5])\n",
            "Embedding(10, 4)\n",
            "Dummy Input (Token IDs): tensor([[2, 7, 1, 2, 4],\n",
            "        [9, 7, 8, 2, 4],\n",
            "        [9, 2, 8, 6, 2]])\n",
            "Output after InputEmbeddings (Token Embeddings): tensor([[[ 1.9724, -2.8027,  3.0187,  1.6199],\n",
            "         [-2.0318, -1.3844, -0.7549, -0.0879],\n",
            "         [ 0.6107, -2.4446,  2.5084,  0.9897],\n",
            "         [ 1.9724, -2.8027,  3.0187,  1.6199],\n",
            "         [ 1.4508, -1.7542, -0.4345,  0.0565]],\n",
            "\n",
            "        [[ 2.5120,  0.2577,  2.9352,  1.2423],\n",
            "         [-2.0318, -1.3844, -0.7549, -0.0879],\n",
            "         [-2.3486,  2.3704,  0.6784, -2.7064],\n",
            "         [ 1.9724, -2.8027,  3.0187,  1.6199],\n",
            "         [ 1.4508, -1.7542, -0.4345,  0.0565]],\n",
            "\n",
            "        [[ 2.5120,  0.2577,  2.9352,  1.2423],\n",
            "         [ 1.9724, -2.8027,  3.0187,  1.6199],\n",
            "         [-2.3486,  2.3704,  0.6784, -2.7064],\n",
            "         [-0.6457,  0.5665,  1.5093,  1.6582],\n",
            "         [ 1.9724, -2.8027,  3.0187,  1.6199]]], grad_fn=<MulBackward0>)\n",
            "with shape torch.Size([3, 5, 4])\n",
            "Output after PositionalEncoding (Positionally Encoded Embeddings): tensor([[[ 2.1916, -2.0030,  3.3541,  2.9110],\n",
            "         [-1.3226, -0.9379, -0.0000,  1.0134],\n",
            "         [ 1.6888, -3.1786,  2.8093,  2.2106],\n",
            "         [ 2.3484, -4.2141,  3.3875,  2.9105],\n",
            "         [ 0.0000, -2.6754, -0.4384,  1.1731]],\n",
            "\n",
            "        [[ 2.7911,  1.3975,  3.2613,  2.4914],\n",
            "         [-1.3226, -0.0000, -0.8276,  1.0134],\n",
            "         [-1.5993,  2.1714,  0.7760, -1.8962],\n",
            "         [ 2.3484, -4.2141,  0.0000,  2.9105],\n",
            "         [ 0.0000, -2.6754, -0.4384,  0.0000]],\n",
            "\n",
            "        [[ 2.7911,  1.3975,  3.2613,  2.4914],\n",
            "         [ 3.1266, -0.0000,  3.3652,  2.9110],\n",
            "         [-1.5993,  2.1714,  0.7760, -1.8962],\n",
            "         [-0.5607, -0.4706,  1.7103,  2.9531],\n",
            "         [ 0.0000, -3.8404,  3.3986,  0.0000]]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ],
      "source": [
        "d_model_ex = 4\n",
        "vocab_size_ex = 10\n",
        "seq_len_ex = 5\n",
        "batch_size_ex = 3\n",
        "dropout_ex = 0.1\n",
        "\n",
        "dummy_input_ex = torch.randint(vocab_size_ex, (batch_size_ex, seq_len_ex))\n",
        "print(dummy_input_ex.shape)\n",
        "\n",
        "# Instantiate the classes with updated parameters\n",
        "input_embeddings_ex = InputEmbeddings(d_model_ex, vocab_size_ex)\n",
        "print(input_embeddings_ex.embeddings)\n",
        "positional_encoding_ex = PositionalEncoding(d_model_ex, seq_len_ex, dropout_ex)\n",
        "\n",
        "# Process the inputs through the classes\n",
        "embedded_input_ex = input_embeddings_ex(dummy_input_ex)\n",
        "encoded_input_ex = positional_encoding_ex(embedded_input_ex)\n",
        "\n",
        "print(\"Dummy Input (Token IDs):\", dummy_input_ex)\n",
        "print(\"Output after InputEmbeddings (Token Embeddings):\", embedded_input_ex)\n",
        "print(\"with shape\", embedded_input_ex.shape)\n",
        "print(\"Output after PositionalEncoding (Positionally Encoded Embeddings):\", encoded_input_ex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "on9j4yJg_oui"
      },
      "source": [
        "for this example, think of 3 sentances of 5, in a vocab list of 10. the embeddings are kind of just like a lookup table for each word in the vocab list thats why its just a constant dim of (vocab size x dim). So we first tokenize the sentance, then add positional encoding (using formula from paper)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMUtBqGrFkxW"
      },
      "outputs": [],
      "source": [
        "# Layer normalization for Add and Norm (from layer normalization paper)\n",
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "  def __init__(self, eps: float = 10**-6) -> None:\n",
        "    super().__init__()\n",
        "    self.eps = eps\n",
        "    # episilon in demoniator of xhat. If sigma happens to be 0 we need this epsilon\n",
        "    self.alpha = nn.Parameter(torch.ones(1)) # makes it learnable (multiplier)\n",
        "    self.bias = nn.Parameter(torch.zeroes(1)) # adder\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(dim=-1, keepdim=True)\n",
        "    std = x.std(dim = -1, keepdim=True)\n",
        "\n",
        "    return self.alpha * (x - mean) / (std + self.eps) + self.bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MI2b_hqBGp0i"
      },
      "outputs": [],
      "source": [
        "# From paper, FFN(x) = max(0, xW1 + b1)W2 + b2\n",
        "\n",
        "class FeedForwardBlock(nn.Module):\n",
        "  def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
        "    super().__init__()\n",
        "    self.linear_1 = nn.Linear(d_model, d_ff) # this is W1 and B1\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.linear_2 = nn.Linear(d_ff, d_model) # this is W2 and B2\n",
        "\n",
        "  def forward(self, x):\n",
        "    # (Batch, Seq_Len, d_model) --> (Batch, Seq_Len, d_model)\n",
        "    return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quick Notes on Attention: \n",
        "- For each word, we create a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices that we trained during the training process. SO, We end up creating a “query”, a “key”, and a “value” projection of each word in the input sentence.\n",
        "- Next we calculate a \"score\" which is just the dot product of the query vector with the key vector of the respective word we’re scoring. So if we’re processing the self-attention for the word in position #1, the first score would be the dot product of q1 and k1, then second q1 * k2, ... q1 * kn\n",
        "- Lastly, we divide the scores by 8 (the square root of the dimension of the key vectors used in the paper — 64. This leads to having more stable gradients and then apply a softmax on all the scores. Can think of it as: \"each of these scores determines how much how much each word will be expressed at this position\"\n",
        "\n",
        "With these scores, we then just multiply each value vector \n",
        "\n",
        "Note: also just need to recall multihead attention (see formula in paper)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymbutRSvHxYO"
      },
      "outputs": [],
      "source": [
        "# Attention\n",
        "\n",
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "  def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.h = h\n",
        "    # make sure dim is dvisible by heads: \n",
        "    assert d_model % h == 0\n",
        "    self.d_k = d_model // h # From paper\n",
        "    self.w_q = nn.Linear(d_model, d_model)\n",
        "    self.w_k = nn.Linear(d_model, d_model)\n",
        "    self.w_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "    self.w_o = nn.Linear(d_model, d_model)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  @staticmethod\n",
        "  def attention(query, key, value, mask, dropout: nn.Dropout):\n",
        "      d_k = query.shape[-1]\n",
        "      # Just apply the formula from the paper\n",
        "      # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
        "      attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "      if mask is not None:\n",
        "          # Write a very low value (indicating -inf) to the positions where mask == 0\n",
        "          attention_scores.masked_fill_(mask == 0, -1e9)\n",
        "      attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n",
        "      if dropout is not None:\n",
        "          attention_scores = dropout(attention_scores)\n",
        "      # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
        "      # return attention scores which can be used for visualization\n",
        "      return (attention_scores @ value), attention_scores\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, q, k, v, mask):\n",
        "    # mask is for words not interacting with others. (before multiplying the attention scores by the value)\n",
        "    query = self.w_q(q)\n",
        "    key = self.w_k(k)\n",
        "    value = self.w_v(v)\n",
        "\n",
        "    # heads\n",
        "    query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1,2) # transpose is because we want 2nd dimension (each head will see seq_len * d_k)\n",
        "    key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "    value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "\n",
        "    x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
        "\n",
        "    # Combine all the heads together\n",
        "    # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
        "    x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
        "\n",
        "    # Multiply by Wo\n",
        "    # (batch, seq_len, d_model) --> (batch, seq_len, d_model)  \n",
        "    return self.w_o(x)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.9.2 ('SLAM')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "a9eb3b4a0bdf5bf03051beabca75ae1aa770bdd8f2b708d500af81de0c2039a4"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
