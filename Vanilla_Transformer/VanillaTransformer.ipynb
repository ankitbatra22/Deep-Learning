{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOW1_zyoCCQA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input Embeddings\n",
        "\n",
        "class InputEmbeddings(nn.Module):\n",
        "  def __init__(self, d_model: int, vocab_size: int):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embeddings = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.embeddings(x) * math.sqrt(self.d_model)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3EJuO2mDEDEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing/Visualizing InputEmbeddings Functionality\n",
        "\n",
        "# hypothetical vocab size, and d_model\n",
        "ex_vocab_size = 10\n",
        "dim_model = 4\n",
        "\n",
        "_embeddings = InputEmbeddings(d_model=dim_model, vocab_size=ex_vocab_size)\n",
        "print(_embeddings.embeddings)\n",
        "\n",
        "# sentance with token ID 1,2\n",
        "tokenized_sentence = torch.LongTensor([1, 2])\n",
        "\n",
        "# Get embeddings for the tokenized sentence\n",
        "embedded_sentence = _embeddings(tokenized_sentence)\n",
        "print(embedded_sentence)\n",
        "print(embedded_sentence.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_5tdT6QKQyn",
        "outputId": "6c364207-c395-45c8-f020-bd8148c8e390"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding(10, 4)\n",
            "tensor([[-1.5787,  3.0195,  3.2364,  0.4048],\n",
            "        [ 3.1898, -2.2366,  0.4090,  1.7561]], grad_fn=<MulBackward0>)\n",
            "torch.Size([2, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Positional Encoding\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    # max length of the sentance (need to create one vector for each position)\n",
        "    self.seq_len = seq_len\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # matrix of shape (seq_len, dim model)\n",
        "    pos_enc = torch.zeros(seq_len, d_model)\n",
        "\n",
        "    # vector for seq_len\n",
        "    position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0)/ d_model))\n",
        "\n",
        "    # from paper, sin to even positions\n",
        "    pos_enc[:, 0::2] = torch.sin(position * div_term)\n",
        "    pos_enc[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "    # we will have a batch of sentances, so we need a new dimension\n",
        "    pos_enc = pos_enc.unsqueeze(0)\n",
        "\n",
        "    self.register_buffer(\"pos_enc\", pos_enc)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # pos enc to every word in sentance\n",
        "    x = x + (self.pos_enc[:, :x.shape[1], :]).requires_grad_(False)\n",
        "    return self.dropout(x)\n"
      ],
      "metadata": {
        "id": "h6DV2OW8KU0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_model_ex = 4\n",
        "vocab_size_ex = 10\n",
        "seq_len_ex = 5\n",
        "batch_size_ex = 3\n",
        "dropout_ex = 0.1\n",
        "\n",
        "dummy_input_ex = torch.randint(vocab_size_ex, (batch_size_ex, seq_len_ex))\n",
        "print(dummy_input_ex.shape)\n",
        "\n",
        "# Instantiate the classes with updated parameters\n",
        "input_embeddings_ex = InputEmbeddings(d_model_ex, vocab_size_ex)\n",
        "print(input_embeddings_ex.embeddings)\n",
        "positional_encoding_ex = PositionalEncoding(d_model_ex, seq_len_ex, dropout_ex)\n",
        "\n",
        "# Process the inputs through the classes\n",
        "embedded_input_ex = input_embeddings_ex(dummy_input_ex)\n",
        "encoded_input_ex = positional_encoding_ex(embedded_input_ex)\n",
        "\n",
        "print(\"Dummy Input (Token IDs):\", dummy_input_ex)\n",
        "print(\"Output after InputEmbeddings (Token Embeddings):\", embedded_input_ex)\n",
        "print(\"with shape\", embedded_input_ex.shape)\n",
        "print(\"Output after PositionalEncoding (Positionally Encoded Embeddings):\", encoded_input_ex)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsKhLbXfx3hh",
        "outputId": "dfc68015-4dfe-4ba7-a179-4d9dc2306bc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 5])\n",
            "Embedding(10, 4)\n",
            "Dummy Input (Token IDs): tensor([[2, 7, 1, 2, 4],\n",
            "        [9, 7, 8, 2, 4],\n",
            "        [9, 2, 8, 6, 2]])\n",
            "Output after InputEmbeddings (Token Embeddings): tensor([[[ 1.9724, -2.8027,  3.0187,  1.6199],\n",
            "         [-2.0318, -1.3844, -0.7549, -0.0879],\n",
            "         [ 0.6107, -2.4446,  2.5084,  0.9897],\n",
            "         [ 1.9724, -2.8027,  3.0187,  1.6199],\n",
            "         [ 1.4508, -1.7542, -0.4345,  0.0565]],\n",
            "\n",
            "        [[ 2.5120,  0.2577,  2.9352,  1.2423],\n",
            "         [-2.0318, -1.3844, -0.7549, -0.0879],\n",
            "         [-2.3486,  2.3704,  0.6784, -2.7064],\n",
            "         [ 1.9724, -2.8027,  3.0187,  1.6199],\n",
            "         [ 1.4508, -1.7542, -0.4345,  0.0565]],\n",
            "\n",
            "        [[ 2.5120,  0.2577,  2.9352,  1.2423],\n",
            "         [ 1.9724, -2.8027,  3.0187,  1.6199],\n",
            "         [-2.3486,  2.3704,  0.6784, -2.7064],\n",
            "         [-0.6457,  0.5665,  1.5093,  1.6582],\n",
            "         [ 1.9724, -2.8027,  3.0187,  1.6199]]], grad_fn=<MulBackward0>)\n",
            "with shape torch.Size([3, 5, 4])\n",
            "Output after PositionalEncoding (Positionally Encoded Embeddings): tensor([[[ 2.1916, -2.0030,  3.3541,  2.9110],\n",
            "         [-1.3226, -0.9379, -0.0000,  1.0134],\n",
            "         [ 1.6888, -3.1786,  2.8093,  2.2106],\n",
            "         [ 2.3484, -4.2141,  3.3875,  2.9105],\n",
            "         [ 0.0000, -2.6754, -0.4384,  1.1731]],\n",
            "\n",
            "        [[ 2.7911,  1.3975,  3.2613,  2.4914],\n",
            "         [-1.3226, -0.0000, -0.8276,  1.0134],\n",
            "         [-1.5993,  2.1714,  0.7760, -1.8962],\n",
            "         [ 2.3484, -4.2141,  0.0000,  2.9105],\n",
            "         [ 0.0000, -2.6754, -0.4384,  0.0000]],\n",
            "\n",
            "        [[ 2.7911,  1.3975,  3.2613,  2.4914],\n",
            "         [ 3.1266, -0.0000,  3.3652,  2.9110],\n",
            "         [-1.5993,  2.1714,  0.7760, -1.8962],\n",
            "         [-0.5607, -0.4706,  1.7103,  2.9531],\n",
            "         [ 0.0000, -3.8404,  3.3986,  0.0000]]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "for this example, think of 3 sentances of 5, in a vocab list of 10. the embeddings are kind of just like a lookup table for each word in the vocab list thats why its just a constant dim of (vocab size x dim). So we first tokenize the sentance, then add positional encoding (using formula from paper)"
      ],
      "metadata": {
        "id": "on9j4yJg_oui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Layer normalization for Add and Norm (from layer normalization paper)\n",
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "  def __init__(self, eps: float = 10**-6) -> None:\n",
        "    super().__init__()\n",
        "    self.eps = eps\n",
        "    # episilon in demoniator of xhat. If sigma happens to be 0 we need this epsilon\n",
        "    self.alpha = nn.Parameter(torch.ones(1)) # makes it learnable (multiplier)\n",
        "    self.bias = nn.Parameter(torch.zeroes(1)) # adder\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(dim=-1, keepdim=True)\n",
        "    std = x.std(dim = -1, keepdim=True)\n",
        "\n",
        "    return self.alpha * (x - mean) / (std + self.eps) + self.bias"
      ],
      "metadata": {
        "id": "nMUtBqGrFkxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# From paper, FFN(x) = max(0, xW1 + b1)W2 + b2\n",
        "\n",
        "class FeedForwardBlock(nn.Module):\n",
        "  def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
        "    super().__init__()\n",
        "    self.linear_1 = nn.Linear(d_model, d_ff) # this is W1 and B1\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.linear_2 = nn.Linear(d_ff, d_model) # this is W2 and B2\n",
        "\n",
        "  def forward(self, x):\n",
        "    # (Batch, Seq_Len, d_model) --> (Batch, Seq_Len, d_model)\n",
        "    return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n"
      ],
      "metadata": {
        "id": "MI2b_hqBGp0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Attention"
      ],
      "metadata": {
        "id": "ymbutRSvHxYO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}